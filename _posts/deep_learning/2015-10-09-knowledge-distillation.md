---
layout: post
category: deep_learning
title: Acceleration and Model Compression
date: 2015-10-09
---

# Papers

**Distilling the Knowledge in a Neural Network**

- intro: NIPS 2014 Deep Learning Workshop
- author: Geoffrey Hinton, Oriol Vinyals, Jeff Dean
- arxiv: [http://arxiv.org/abs/1503.02531](http://arxiv.org/abs/1503.02531)
- blog: [http://fastml.com/geoff-hintons-dark-knowledge/](http://fastml.com/geoff-hintons-dark-knowledge/)
- notes: [https://github.com/dennybritz/deeplearning-papernotes/blob/master/notes/distilling-the-knowledge-in-a-nn.md](https://github.com/dennybritz/deeplearning-papernotes/blob/master/notes/distilling-the-knowledge-in-a-nn.md)

**Deep Model Compression: Distilling Knowledge from Noisy Teachers**

- arxiv: [https://arxiv.org/abs/1610.09650](https://arxiv.org/abs/1610.09650)
- github: [https://github.com/chengshengchan/model_compression](https://github.com/chengshengchan/model_compression)]

**Like What You Like: Knowledge Distill via Neuron Selectivity Transfer**

- intro: TuSimple
- arxiv: [https://arxiv.org/abs/1707.01219](https://arxiv.org/abs/1707.01219)
- github: [https://github.com/TuSimple/neuron-selectivity-transfer](https://github.com/TuSimple/neuron-selectivity-transfer)

**Learning Loss for Knowledge Distillation with Conditional Adversarial Networks**

[https://arxiv.org/abs/1709.00513](https://arxiv.org/abs/1709.00513)

**Data-Free Knowledge Distillation for Deep Neural Networks**

[https://arxiv.org/abs/1710.07535](https://arxiv.org/abs/1710.07535)

**Knowledge Projection for Deep Neural Networks**

[https://arxiv.org/abs/1710.09505](https://arxiv.org/abs/1710.09505)

**Moonshine: Distilling with Cheap Convolutions**

[https://arxiv.org/abs/1711.02613](https://arxiv.org/abs/1711.02613)

**model_compression: Implementation of model compression with knowledge distilling method**

- github: [https://github.com/chengshengchan/model_compression](https://github.com/chengshengchan/model_compression)

**Neural Network Distiller**

- intro: Neural Network Distiller: a Python package for neural network compression research
- project page: [https://nervanasystems.github.io/distiller/](https://nervanasystems.github.io/distiller/)
- github: [https://github.com/NervanaSystems/distiller](https://github.com/NervanaSystems/distiller)

**Knowledge Distillation in Generations: More Tolerant Teachers Educate Better Students**

- intro: The Johns Hopkins University
- arxiv: [https://arxiv.org/abs/1805.05551](https://arxiv.org/abs/1805.05551)

**Improving Knowledge Distillation with Supporting Adversarial Samples**

[https://arxiv.org/abs/1805.05532](https://arxiv.org/abs/1805.05532)

**Recurrent knowledge distillation**

- intro: ICIP 2018
- arxiv: [https://arxiv.org/abs/1805.07170](https://arxiv.org/abs/1805.07170)

**Knowledge Distillation by On-the-Fly Native Ensemble**

[https://arxiv.org/abs/1806.04606](https://arxiv.org/abs/1806.04606)

**Improved Knowledge Distillation via Teacher Assistant: Bridging the Gap Between Student and Teacher**

- intro: Washington State University & DeepMind
- arxiv: [https://arxiv.org/abs/1902.03393](https://arxiv.org/abs/1902.03393)

**Similarity-Preserving Knowledge Distillation**

- intro: ICCV 2019
- arxiv: [https://arxiv.org/abs/1907.09682](https://arxiv.org/abs/1907.09682)

**Highlight Every Step: Knowledge Distillation via Collaborative Teaching**

[https://arxiv.org/pdf/1907.09643.pdf](https://arxiv.org/pdf/1907.09643.pdf)

**Ensemble Knowledge Distillation for Learning Improved and Efficient Networks**

[https://arxiv.org/abs/1909.08097](https://arxiv.org/abs/1909.08097)

**Revisit Knowledge Distillation: a Teacher-free Framework**

- arxiv: [https://arxiv.org/abs/1909.11723](https://arxiv.org/abs/1909.11723)
- github: [https://github.com/yuanli2333/Teacher-free-Knowledge-Distillation](https://github.com/yuanli2333/Teacher-free-Knowledge-Distillation)

**On the Efficacy of Knowledge Distillation**

- intro: Cornell University
- arxiv: [https://arxiv.org/abs/1910.01348](https://arxiv.org/abs/1910.01348)

**Training convolutional neural networks with cheap convolutions and online distillation**

- arxiv: [https://arxiv.org/abs/1909.13063](https://arxiv.org/abs/1909.13063)
- github: [https://github.com/EthanZhangYC/OD-cheap-convolution](https://github.com/EthanZhangYC/OD-cheap-convolution)

**Knowledge Representing: Efficient, Sparse Representation of Prior Knowledge for Knowledge Distillation**

[https://arxiv.org/abs/1911.05329](https://arxiv.org/abs/1911.05329)

**Preparing Lessons: Improve Knowledge Distillation with Better Supervision**

- intro: Xiâ€™an Jiaotong University & Meituan
- keywords: Knowledge Adjustment (KA), Dynamic Temperature Distillation (DTD)
- arxiv: [https://arxiv.org/abs/1911.07471](https://arxiv.org/abs/1911.07471)

**QKD: Quantization-aware Knowledge Distillation**

[https://arxiv.org/abs/1911.12491](https://arxiv.org/abs/1911.12491)

# resources

**Awesome Knowledge-Distillation**

[https://github.com/FLHonker/Awesome-Knowledge-Distillation](https://github.com/FLHonker/Awesome-Knowledge-Distillation)
